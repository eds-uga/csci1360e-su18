{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "q1_prompt",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Question 1\n",
    "\n",
    "In this question, you'll be doing some basic processing on four books: *Moby Dick*, *War and Peace*, *The King James Bible*, and *The Complete Works of William Shakespeare*. Each of these texts are available for free on the [Project Gutenberg](https://www.gutenberg.org/ebooks/) website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "q1a_prompt",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part A\n",
    "\n",
    "Write a function, `read_book`, which takes the name of the text file containing the book's content, and returns a single string containing the content.\n",
    "\n",
    " - input: a single string indicating the name of the text file with the book\n",
    " - output: a single string of the entire book\n",
    " \n",
    "Your function should be able to handle file-related errors gracefully (this means a `try`/`except` block!). If an error occurs, just return `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1a",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q1a_test1",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert read_book(\"queen_jean_bible.txt\") is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q1a_test2",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert read_book(\"complete_shakspeare.txt\") is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q1a_test3",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "book1 = read_book(\"moby_dick.txt\")\n",
    "assert len(book1) == 1238567"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q1a_test4",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "book2 = read_book(\"war_and_peace.txt\")\n",
    "assert len(book2) == 3224780"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1b_prompt",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part B\n",
    "\n",
    "Write a function, `word_counts`, which takes a single string as input (containing an entire book), and returns as output a dictionary of word counts.\n",
    "\n",
    "Don't worry about handling punctuation, but definitely handle whitespace (spaces, tabs, newlines). Also make sure to handle capitalization, and throw out any words with a length of 2 or less. No other \"preprocessing\" requirements outside these.\n",
    "\n",
    "You are welcome to use the [`collections.defaultdict` dictionary](https://docs.python.org/3/library/collections.html#collections.defaultdict) for tracking word counts, but no other built-in Python packages or functions for counting. `defaultdict` dictionaries are nice because if you try to update an element that does not yet exist in the dictionary, it automatically creates it and initializes it to a \"default\" value (hence, the name); this differs from the standard built-in Python dictionaries, in that this operation would cause your program to crash!\n",
    "\n",
    "If you want to use defaultdictionaries but aren't sure how, **please ask on Slack!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q1b_test1",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert 0 == len(word_counts(\"\").keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q1b_test2",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert 1 == word_counts(\"hi there\")[\"there\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q1b_test3",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "kj = word_counts(open(\"king_james_bible.txt\", \"r\").read())\n",
    "assert 23 == kj[\"devil\"]\n",
    "assert 4 == kj[\"leviathan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q1b_test4",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "wp = word_counts(open(\"war_and_peace.txt\", \"r\").read())\n",
    "assert 30 == wp[\"devil\"]\n",
    "assert 86 == wp[\"soul\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1c_prompt",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part C\n",
    "\n",
    "Write a function, `total_words`, which takes as input a string containing the contents of a book, and returns as output the integer count of the *total number of words* (this is NOT unique words, but *total* words).\n",
    "\n",
    "Same rules apply as in Part B with respect to what constitutes a \"word\" (capitalization, punctuation, splitting, etc), but you are welcome to use your Part B solution in answering this question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1c",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q1c_test1",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    words = total_words(\"\")\n",
    "except:\n",
    "    assert False\n",
    "else:\n",
    "    assert words == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q1c_test2",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert 11 == total_words(\"The brown fox jumped over the lazy cat.\\nTwice.\\nMMyep. Twice.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q1c_test3",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert 681216 == total_words(open(\"king_james_bible.txt\", \"r\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q1c_test4",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert 729531 == total_words(open(\"complete_shakespeare.txt\", \"r\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1d_prompt",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part D\n",
    "\n",
    "Write a function, `unique_words`, which takes as input a string containing the full contents of a book, and returns an integer count of the number of *unique* words in the book.\n",
    "\n",
    "Same rules apply as in Part B with respect to what constitutes a \"word\" (capitalization, punctuation, splitting, etc), but you are welcome to use your Part B solution in answering this question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1d",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q1d_test1",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    words = total_words(\"\")\n",
    "except:\n",
    "    assert False\n",
    "else:\n",
    "    assert words == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q1d_test2",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert 9 == unique_words(\"The brown fox jumped over the lazy cat.\\nTwice.\\nMMyep. Twice.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q1d_test3",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert 31586 == unique_words(open(\"moby_dick.txt\", \"r\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q1d_test4",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert 40021 == unique_words(open(\"war_and_peace.txt\", \"r\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1e_prompt",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part E\n",
    "\n",
    "Write a function, `global_vocabulary`, which takes a *list of strings* as its single input argument. Each string element of the list contains the contents of an entire book. The output of the function should be a list or set of *unique words* that comprise the full vocabulary of terms present across all the books that are passed to the function.\n",
    "\n",
    "For example, if I have the following code:\n",
    "\n",
    "```\n",
    "book1 = \"This is the entire content of a book.\"\n",
    "book2 = \"Here's another book.\"\n",
    "book3 = \"What is this?\"\n",
    "books = [book1, book2, book3]  # list of strings\n",
    "\n",
    "vocabulary = global_vocabulary(books)\n",
    "```\n",
    "\n",
    "this should return a list or set containing the words:\n",
    "\n",
    "```\n",
    "{'a',\n",
    " 'another',\n",
    " 'book.',\n",
    " 'content',\n",
    " 'entire',\n",
    " \"here's\",\n",
    " 'is',\n",
    " 'of',\n",
    " 'the',\n",
    " 'this',\n",
    " 'this?',\n",
    " 'what'}\n",
    "```\n",
    "\n",
    "The words should be in increasing lexicographic order (aka, standard alphabetical order), and all the preprocessing steps required in previous sections should be used. As such, you are welcome to use your `word_counts` function from Part B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q1e_test1",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "doc1 = \"This is a sentence.\"\n",
    "doc2 = \"This is another sentence.\"\n",
    "doc3 = \"What is this?\"\n",
    "\n",
    "assert set([\"another\", \"sentence.\", \"this\", \"this?\", \"what\"]) == set(global_vocabulary([doc1, doc2, doc3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q1e_test2",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert 31586 == len(global_vocabulary([open(\"moby_dick.txt\", \"r\").read()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q1e_test3",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert 40021 == len(global_vocabulary([open(\"war_and_peace.txt\", \"r\").read()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q1e_test4",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "kj = open(\"king_james_bible.txt\", \"r\").read()\n",
    "wp = open(\"war_and_peace.txt\", \"r\").read()\n",
    "md = open(\"moby_dick.txt\", \"r\").read()\n",
    "cs = open(\"complete_shakespeare.txt\", \"r\").read()\n",
    "\n",
    "assert 118503 == len(global_vocabulary([kj, wp, md, cs]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1f_prompt",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part F\n",
    "\n",
    "Write a function, `featurize`, which takes a *list of strings* as its lone input argument: each element is a string with the contents of an entire book. The output of this function is a 2D NumPy array of counts, where the rows are the documents/books (i.e., one row per element!) and the columns are the counts for all the words in the global vocabulary.\n",
    "\n",
    "For instance, if I pass a two-string list to `featurize` that collectively have 50 unique words between the two strings, the output matrix should have shape `(2, 50)`: the first row will be the respective counts of the words in that document, and same with the second row.\n",
    "\n",
    "The rows (documents) should be in the same ordering as they're given in the function's argument list, and the columns (words) should be in increasing lexicographic order (aka alphabetic order). You are welcome to use your function from Part B, and from Part E (Part E's function would be particularly useful here, since that pretty much does all the heavy lifting; you'd just need to convert that dictionary to a matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1f",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q1f_test1",
     "locked": true,
     "points": 8,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "kj = open(\"king_james_bible.txt\", \"r\").read()\n",
    "wp = open(\"war_and_peace.txt\", \"r\").read()\n",
    "\n",
    "matrix = featurize([kj, wp])\n",
    "assert 2 == matrix.shape[0]\n",
    "assert 63889 == matrix.shape[1]\n",
    "assert 2 == int(matrix[:, 836].sum())\n",
    "assert 16 == int(matrix[:, 62655].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q1f_test2",
     "locked": true,
     "points": 8,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "kj = open(\"king_james_bible.txt\", \"r\").read()\n",
    "wp = open(\"war_and_peace.txt\", \"r\").read()\n",
    "md = open(\"moby_dick.txt\", \"r\").read()\n",
    "cs = open(\"complete_shakespeare.txt\", \"r\").read()\n",
    "\n",
    "matrix = featurize([kj, wp, md, cs])\n",
    "assert 4 == matrix.shape[0]\n",
    "assert 118503 == matrix.shape[1]\n",
    "assert 3 == int(matrix[:, 103817].sum())\n",
    "assert 1 == int(matrix[:, 71100].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1g_prompt",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part G\n",
    "\n",
    "Write a function, `probability`, which takes three arguments:\n",
    " - an integer, indicating the index of the word we're interested in computing the probability for (i.e., column number)\n",
    " - a 2D NumPy matrix of word counts, where the rows are the documents, and the columns are the words\n",
    " - an *optional* integer, indicating the specific document in which we want to compute the probability of the word (default is all documents)\n",
    " \n",
    "This function is the implementation of $P(w)$ for some word $w$. By default, this is probability of word $w$ over our entire dataset. However, by specifying an optional integer, we can specify a conditional probability $P(w | d)$. In this case, we're asking for the probability of word $w$ *given* some specific document $d$.\n",
    "\n",
    "Your function should return the probability, a floating-point value between 0 and 1. It should be able to handle the case where the specified word index is out of bounds (resulting probability of 0), as well as the case where the document index is out of bounds (also a probability of 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1g",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q1g_test1",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "matrix = np.load(\"lut.npy\")\n",
    "\n",
    "np.testing.assert_allclose(0.068569417725812987, probability(104088, matrix))\n",
    "np.testing.assert_allclose(0.012485067486917144, probability(54096, matrix))\n",
    "np.testing.assert_allclose(0.0073786475907416712, probability(21668, matrix))\n",
    "np.testing.assert_allclose(0.0, probability(66535, matrix), rtol = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q1g_test2",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "matrix = np.load(\"lut.npy\")\n",
    "\n",
    "np.testing.assert_allclose(0.012404288801202555, probability(54096, matrix, 0))\n",
    "np.testing.assert_allclose(0.0077914081371666744, probability(21668, matrix, 1))\n",
    "np.testing.assert_allclose(0.0094279749592546449, probability(117297, matrix, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1h_prompt",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part H\n",
    "\n",
    "Let's assume the four books you've analyzed are now going to constitute your \"background\" data. \"Background\" data is a concept that, in theory, allows you to identify important words: if you analyze a *new* book, you can compare its word counts to those in your \"background\" dataset. Any words in the new book that occur a lot more or a lot less frequently than in the background data could be considered \"important\" in some sense.\n",
    "\n",
    "Let's say you receive a new book: [*Guns of the South*](https://www.amazon.com/Guns-South-Harry-Turtledove/dp/0345384687). You want to compare its word counts to those in your \"background\". However, you quickly run into a problem: there are words in *Guns of the South* that **do not exist at all** in your background dataset--words like \"Abraham\" and \"Lincoln\" only show up in the new book, but never in your \"background\". This is extremely problematic, since now you'd potentially be dividing by 0 to gauge the relative importance of the words in *Guns of the South*.\n",
    "\n",
    "Can you suggest a preprocessing step that might help? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q1h",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
